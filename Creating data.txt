Creating a **synthetic data catalog and tables** in BigQuery is useful for testing pipelines, analytics, or training models when real data can't be used.

Here's a clear step-by-step approach to:

---

## ‚úÖ Step 1: Create a Dataset (Catalog)

```sql
-- Create a dataset called "synthetic_data"
CREATE SCHEMA `your-project-id.synthetic_data`
OPTIONS(
  location = 'US'
);
```

> üîÅ Replace `your-project-id` with your actual GCP project ID
> üìç Adjust `location` to match your region (`US`, `asia-south1`, etc.)

---

## ‚úÖ Step 2: Create a Synthetic Table (Example: Users)

```sql
CREATE TABLE `your-project-id.synthetic_data.users_synthetic` AS
SELECT
  GENERATE_UUID() AS user_id,
  CONCAT('user_', CAST(ROUND(RAND()*10000) AS STRING)) AS username,
  ['basic', 'premium', 'enterprise'][ORD(FLOOR(RAND() * 3))] AS account_type,
  DATE_SUB(CURRENT_DATE(), INTERVAL CAST(FLOOR(RAND()*1000) AS INT64) DAY) AS signup_date,
  IF(RAND() < 0.5, TRUE, FALSE) AS is_active
FROM UNNEST(GENERATE_ARRAY(1, 1000)) AS x;
```

üîπ This will generate **1000 fake users** with randomized:

* IDs
* Usernames
* Account types
* Signup dates
* Active status

---

## ‚úÖ Step 3: Create Another Synthetic Table (Example: Transactions)

```sql
CREATE TABLE `your-project-id.synthetic_data.transactions_synthetic` AS
SELECT
  GENERATE_UUID() AS txn_id,
  GENERATE_UUID() AS user_id,  -- link this if needed to users
  DATE_SUB(CURRENT_DATE(), INTERVAL CAST(FLOOR(RAND()*365) AS INT64) DAY) AS txn_date,
  ROUND(RAND()*5000, 2) AS amount,
  ['success', 'pending', 'failed'][ORD(FLOOR(RAND() * 3))] AS status
FROM UNNEST(GENERATE_ARRAY(1, 5000)) AS x;
```

---

## üß™ Tips for Using Synthetic Data

| Goal                  | Approach                                    |
| --------------------- | ------------------------------------------- |
| Join testing          | Use same `user_id` pattern in both tables   |
| Partitioning test     | Add `DATE` columns for partitioning         |
| ML training           | Add numeric/categorical features            |
| Data pipeline testing | Insert synthetic delays, NULLs, or outliers |

---

## ‚úÖ Export/Use with Other Tools

* Use `EXPORT DATA` to GCS for pipeline testing
* Use `bq load` or Python script to automate creation
* Use `dbt` or Dataform for repeatable generation logic

---

Would you like a version that creates synthetic **IoT data**, **health records**, or **logs** instead?
