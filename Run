"""
Airflow DAG to copy a BigQuery table from one dataset to another

Source:
    sandbox-corp-odin-devl-f930.corp_sec.t_badge_events
Target:
    sandbox-corp-odin-dev1-f930.rto.t_badge_events

Author: ChatGPT (for you!)
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import timedelta
import logging

from google.cloud import bigquery

# Setup logging
LOGGER = logging.getLogger("airflow.task")
LOGGER.setLevel(logging.INFO)

# Define default DAG arguments
default_args = {
    'depends_on_past': False,
    'start_date': days_ago(1),
    'retries': 2,
    'retry_delay': timedelta(minutes=2)
}

# Instantiate the DAG
with DAG(
    dag_id='bq_copy_t_badge_events_dag',
    default_args=default_args,
    description='Copy BigQuery table from corp_sec to rto dataset',
    schedule_interval=None,
    catchup=False,
    tags=['bigquery', 'copy', 'composer']
) as dag:

    def copy_bq_table(**context):
        client = bigquery.Client()

        source_table = "sandbox-corp-odin-devl-f930.corp_sec.t_badge_events"
        target_table = "sandbox-corp-odin-dev1-f930.rto.t_badge_events"

        LOGGER.info(f"✅ Starting copy from {source_table} to {target_table}")

        copy_job = client.copy_table(
            sources=source_table,
            destination=target_table,
            job_config=bigquery.CopyJobConfig(
                write_disposition="WRITE_TRUNCATE"
            )
        )

        copy_job.result()

        LOGGER.info(f"✅ Copy completed successfully: {source_table} → {target_table}")

    # Define the PythonOperator task
    copy_table_task = PythonOperator(
        task_id='copy_bigquery_table',
        python_callable=copy_bq_table,
        provide_context=True
    )

    copy_table_task
